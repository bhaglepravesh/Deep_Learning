{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf745970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerPerceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.01):\n",
    "        self.weights = np.random.rand(input_size)  # Initialize weights with random values\n",
    "        self.bias = np.random.rand()  # Initialize bias with a random value\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        linear_output = np.dot(inputs, self.weights) + self.bias\n",
    "        output = self.activation(linear_output)\n",
    "        return output\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, target in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                error = target - prediction\n",
    "                self.weights += self.learning_rate * error * inputs\n",
    "                self.bias += self.learning_rate * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f7aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05cf2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = SingleLayerPerceptron(input_size=2, learning_rate=0.1)\n",
    "perceptron.train(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1876c77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [0 0], Target: 0, Predicted: 0\n",
      "Inputs: [0 1], Target: 0, Predicted: 0\n",
      "Inputs: [1 0], Target: 0, Predicted: 0\n",
      "Inputs: [1 1], Target: 1, Predicted: 1\n"
     ]
    }
   ],
   "source": [
    "for inputs, target in zip(X, y):\n",
    "    prediction = perceptron.predict(inputs)\n",
    "    print(f\"Inputs: {inputs}, Target: {target}, Predicted: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861df226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "077b5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76860177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784) / 255.0\n",
    "X_test = X_test.reshape(-1, 784) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e6fac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential([\n",
    "    Dense(units=10, activation='softmax', input_shape=(784,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d57b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = SGD(learning_rate=0.1)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a78add46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4171 - accuracy: 0.8850\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3144 - accuracy: 0.9118\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2974 - accuracy: 0.9168\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2881 - accuracy: 0.9192\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2822 - accuracy: 0.9207\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2778 - accuracy: 0.9222\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2742 - accuracy: 0.9237\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2716 - accuracy: 0.9247\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2689 - accuracy: 0.9255\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2673 - accuracy: 0.9255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f406114ed0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a146629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2733 - accuracy: 0.9217\n",
      "Test loss: 0.2733, Test accuracy: 0.9217\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e32e45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras - to define and train a signle-layer perceptron\n",
    "# Load and Preprocess the dataset - Sequential model with Single Dense Layer\n",
    "# Softmax activation function - Classification Problems\n",
    "# Compile model - SGD Optimizer and sparse categorical cross-entropy loss\n",
    "# Multi-Class Classification Problems\n",
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d048f2b",
   "metadata": {},
   "source": [
    "# feedforward algorithm for a simple single-layer perceptron (SLP) using the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee0fde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb32c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b30e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values to a range of 0 to 1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c9b0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the step activation function\n",
    "def step_function(z):\n",
    "    return 1 if z >= 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c95b21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias\n",
    "n_features = X_train.shape[1] * X_train.shape[2]\n",
    "weights = np.zeros(n_features)\n",
    "bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f306564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random sample for testing\n",
    "sample_idx = np.random.randint(0, X_train.shape[0])\n",
    "input_sample = X_train[sample_idx].reshape(-1)  # Flatten the image\n",
    "true_label = y_train[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "402e6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted sum\n",
    "weighted_sum = np.dot(input_sample, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b893b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the step activation function\n",
    "output = step_function(weighted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bcda39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sample:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01960784\n",
      "  0.09411765 0.46666667 0.54509804 0.54509804 0.83529412 0.54117647\n",
      "  0.01960784 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.01960784 0.5372549\n",
      "  0.99215686 0.99215686 0.99215686 0.98039216 0.81176471 0.68627451\n",
      "  0.06666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.42352941 0.99215686\n",
      "  0.99215686 0.99215686 0.87058824 0.24705882 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54509804 0.99215686\n",
      "  0.99215686 0.52941176 0.09803922 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54509804 0.99215686\n",
      "  0.99215686 0.18039216 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54509804 0.99607843\n",
      "  0.99607843 0.18039216 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54509804 0.99215686\n",
      "  0.99215686 0.18039216 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54509804 0.99215686\n",
      "  0.99215686 0.18039216 0.25098039 0.52156863 0.43921569 0.0745098\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54509804 0.99215686\n",
      "  0.99215686 0.84705882 0.97647059 0.99607843 0.99215686 0.8627451\n",
      "  0.65098039 0.36470588 0.16862745 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09019608 0.83137255\n",
      "  0.99215686 0.99215686 0.99215686 0.99607843 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.90980392 0.16470588 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.59607843\n",
      "  0.99607843 0.99607843 0.95686275 0.45882353 0.45490196 0.80784314\n",
      "  0.99607843 0.99607843 0.90980392 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14901961\n",
      "  0.81176471 0.81176471 0.23529412 0.         0.         0.10196078\n",
      "  0.87843137 0.99215686 0.82352941 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.81568627 0.99215686 0.45490196 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.10196078\n",
      "  0.88235294 0.99215686 0.29019608 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2745098\n",
      "  0.99215686 0.99215686 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.72941176\n",
      "  0.99607843 0.99607843 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37647059 0.63921569 0.55294118\n",
      "  0.05098039 0.         0.         0.         0.03529412 0.77254902\n",
      "  0.99215686 0.70196078 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.10196078 0.92941176 1.         0.99215686\n",
      "  0.79607843 0.35686275 0.14901961 0.02745098 0.61176471 0.99215686\n",
      "  0.99215686 0.21176471 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.23137255 0.96078431 0.99607843 0.99215686\n",
      "  0.99215686 0.99215686 0.91372549 0.83529412 0.99215686 0.99215686\n",
      "  0.69411765 0.01568627 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.08235294 0.54117647 0.5372549\n",
      "  0.87058824 0.99215686 0.99215686 0.99607843 0.99215686 0.6627451\n",
      "  0.1372549  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "True Label: 5\n",
      "Weighted Sum: 0.0\n",
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(\"Input Sample:\")\n",
    "print(X_train[sample_idx])  # Display the original image\n",
    "print(\"True Label:\", true_label)\n",
    "print(\"Weighted Sum:\", weighted_sum)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf19b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
